---
title: "STA 380 Take-Home Exam"
author: "Michael Sparkman, Jenny Tseng, Brandon Whiteley, Qinpei Zou"
date: "8/6/2019 - 8/19/2019"
output: md_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Problem 1. Visual story telling part 1: green buildings

```{r warning = FALSE, message= FALSE}
rm(list=ls())
green = read.csv("greenbuildings.csv")
# summary(green)
library(mosaic)
library(tidyverse)
```

We started by checking potential confounders in the dataset and diving into those that appear to have the potential, namely "age" and "class_a." 
```{r, fig.align='center', warning = FALSE, message=FALSE}
# Check whether age is a plausible confounder
g = ggplot(green)
g + geom_histogram(aes(x = age, y=stat(density))) +
  facet_grid(green_rating~.) +
  labs(title="Density Distribution of Property Ages",
       subtitle = "Non-Green vs. Green Properties",
       x ="Property Age",
       y = "Density")
```

**Comments:** Non-green buildings have two "clusters" of ages - one similar to those of green buildings (< 50), the other between 75 and 125. Given that green buildings do not have the older "cluster" and new buildings tend to have higher rents, Age of the properties is a confounding variable.

```{r, fig.align='center'}
# Try to hold age roughly constant
# define some age groupings
green = mutate(green,
               agecat = cut(age, c(0, 10, 25, 50, 75, 200),include.lowest = TRUE))

# compare rent within age groupings
rentByAge = green %>%
  group_by(agecat, green_rating) %>%
  summarize(median_rent = median(Rent), n=n()) # do median to eliminate the effects of outlier

ggplot(rentByAge) +
  geom_bar(stat = 'identity', aes(x = agecat,
                                  y = median_rent,
                                  fill = factor(green_rating,)),
           position = 'dodge') +
  labs(title="Median Property Rent by Property Age Categories",
       subtitle = "Non-Green vs. Green Properties",
       x ="Property Age Category",
       y = "Median Property Rent ($/sqft)") +
  scale_fill_discrete(name = "Building Rating",
                      labels=c("Non-Green-Rated", "Green-Rated"))
```

**Comment**: Holding property age constant, the median rents for green-rated buildings older than 10 years in age do appear to be higher than those for non-green-rated buildings.

```{r, fig.align='center'}
# Check whether class_a is a plausible confounder
g = ggplot(green)
g + geom_bar(aes(factor(class_a), fill =  factor(green_rating)),
           position = 'dodge') +
  labs(title="Numbers of Non-Class A vs. Class A Buildings",
       x ="Non-Class A / Class A",
       y = "Counts") +
  scale_fill_discrete(name = "Building Rating",
                      labels=c("Non-Green-Rated", "Green-Rated"))

```  

**Comment:** Class A seems like a potential confounder as more green buildings appear to be classified as Class A. Higher-quality properties tend to translate into higher prices. So the higher average rent associated with green buildings may be at least partially attributed to building qualities.

```{r, fig.align='center'}
# Try to hold class-A roughly constant
rentByclassA = green %>%
  group_by(class_a, green_rating) %>%
  summarize(median_rent = median(Rent), n=n())
ggplot(rentByclassA) +
  geom_bar(stat = 'identity', aes(x = factor(class_a),
                                  y = median_rent,
                                  fill = factor(green_rating,)),
           position = 'dodge') +
  labs(title="Median Rents for Non-Class A vs. Class A Properties",
       subtitle = "between Non-Green and Green Properties",
       x ="Non-Class A / Class A",
       y = "Average Property Rent ($/sqft)") +
  scale_fill_discrete(name = "Building Rating",
                      labels=c("Non-Green-Rated", "Green-Rated"))
```

**Comment:** For Class A buildings, the rents of green buildings do not appear to be statistically different from non-green buildings. However, for non-Class A buildings, the green-rated buildings do appear to have higher rents.


**Concluding Thoughts:** One should not simply conclude that green buildings have higher rents because they are green. In reality, other factors such as the age and the quality of a property also affect rents. That said, as seen from the graphs above, when holding the property age constant, older green buildings do appear to be of higher rents than non-green buildings, though their rents may be lower during the first ten years post construction. Moreover, if the client intends to build a class A property, there is no significant difference in rent between green and non-green buildings. In conclusion, if the client plans to build a non-Class-A building, they may expect to charge higher rents in the long run, but not in the short term.


# Problem 2. Visual story telling part 2: flights at ABIA

```{r warning=FALSE, message=FALSE}
# Setup
rm(list=ls())
library(mosaic)
library(tidyverse)

ABIA = read.csv("ABIA.csv")

# Get Airline names instead of IATACode
airlinecode = read.csv("IATA-Airline.csv", sep = ',', header = TRUE)
ABIAAirline = merge(ABIA, airlinecode, by.x = "UniqueCarrier", by.y = "ï..IATA")
```

**Thesis:** We would like to explore outbound flight cancellations from ABIA in 2008. Are there particular airlines that had more cancellations? Were these cancellations out of their control? We start by level setting the flight volume by airline, then diving into the cancellation volume, cancellations as a percentage of total flights, and finally reasons for cancellations. 

```{r, warning = FALSE, message = FALSE, fig.align='center'}
# Dummy variable setup
n = dim(ABIAAirline)[1]
CanCarrier = rep(0,n)
CanCarrier[ABIAAirline$CancellationCode=='A']=1
CanWeather = rep(0,n)
CanWeather[ABIAAirline$CancellationCode=='B']=1
CanNAS = rep(0,n)
CanNAS[ABIAAirline$CancellationCode=='C']=1

ABIAAirline['CanCarrier'] = CanCarrier
ABIAAirline['CanWeather'] = CanWeather
ABIAAirline['CanNAS'] = CanNAS

FlightCancellations = ABIAAirline %>%
  filter(Origin == 'AUS') %>%
  group_by(AirlineName) %>%
  summarize(flight_count = n() , # Flights out of AUS by airline
            cancelled_count = sum(Cancelled), # Cancels out of AUS by airline
            cancelRatio = cancelled_count / flight_count, # % cancels out of AUS by airline
            CarrierCancellation = sum(CanCarrier), # reasons for cancels
            WeatherCancellation = sum(CanWeather),
            NASCancellation = sum(CanNAS),
            CarrierCanPerc = round(CarrierCancellation / cancelled_count,3),
            WeatherCanPerc = round(WeatherCancellation / cancelled_count,3),
            NASCanPerc = round(NASCancellation / cancelled_count,3))

# Plot flight count
ggplot(FlightCancellations[,1:2]) + geom_bar(stat = 'identity',
                        aes(x = reorder(AirlineName, -flight_count),
                            y = flight_count)) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title="Number of Outbound Flights from AUS, by Airline - 2008",
       x ="Airline",
       y = "Number of Outbound Flights Scheduled")

# Plot cancel count
ggplot(FlightCancellations[,c(1,3)]) + geom_bar(stat = 'identity',
                        aes(x = reorder(AirlineName, -cancelled_count),
                            y = cancelled_count)) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title="Number of Outbound Flights Cancelled, by Airline - 2008",
       x ="Airline",
       y = "Number of Flights Cancelled")

# Plot cancel %
ggplot(FlightCancellations[,c(1,4)]) + geom_bar(stat = 'identity',
                        aes(x = reorder(AirlineName, -cancelRatio),
                            y = cancelRatio)) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title="Percentage of Outbound Flights Cancelled, by Airline - 2008",
       x ="Airline",
       y = "Percentage of Flights Cancelled")

# Get Plot 3's order for Plot 4 
Xorder = reorder(FlightCancellations$AirlineName, -FlightCancellations$cancelRatio)

# Plot cancellation reasons by airline
library(reshape2)
df <- melt(FlightCancellations[c(1,8,9,10)], id.vars = "AirlineName")
df$value[is.na(df$value)] <- 0
vec = levels(Xorder)
df$AirlineName <- factor(df$AirlineName, levels=vec) # reordering the columns as # of delays out
ggplot(df, aes(x = AirlineName,
               y = value,
               fill = variable)) +
  geom_bar(stat = 'identity') +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title="AUS Outbound Flight Cancellation Reasons, by Airline - 2008",
       x ="Airline", 
       y = "Percentage of Cancelled Flights") +
  scale_fill_discrete(name = "Cancellation Reason",
                      labels=c("Carrier", "Weather", "NAS"))

```

**Concluding Thoughts:** As one can see from the first and second plots, while Southwest flew the most flights out of Austin in 2008 (~70% more than second place American Airlines), its cancellation volume is about a third of that of American. In fact, when looking at percentage of flights cancelled (third plot), Southwest was the fourth lowest. Lastly, among the five airlines that had the most cancellations by percentage, only American Eagle and Comair had less than half of the flights cancelled due to factors outside of their control. Mesa especially had over 80% of its cancellations due to carrier-controlled factors.

**Food for thought:** Among those with relatively high cancellation percentages:

- Pinnacle and Comair are no longer operating
- American Eagle is part of American Airlines
- Mesa operates for American Airlines and United Airlines
- SkyWest operates for American, United, and Alaska


# Problem 3. Portfolio modeling

## Portfolio 1:

- Diversified ETF: GCE (Clymore CEF GS Connect ETN)
- Health & Biotech: IHI (iShares US Medical Devices ETF)
- S. America: FBZ (First Trust Brazil Alpha DEX Fund)
```{r, warning=FALSE, message=FALSE}
rm(list=ls())
library(mosaic)
library(quantmod)
library(foreach)

# Import ETF
myETF = c("GCE","IHI","FBZ")
getSymbols(myETF, from = "2014-08-01") # Last five years of data
```
```{r}
# Adjust for splits and dividends
for(ticker in myETF) {
	expr = paste0(ticker, "a = adjustOHLC(", ticker, ")")
	eval(parse(text=expr))
}
# head(GCE)
```
```{r}
# Combine, in col, all the returns (close-to-close, 24-hr, changes) in a matrix
all_returns = cbind(ClCl(GCE),
								ClCl(IHI),
								ClCl(FBZ))
# head(all_returns)
all_returns = as.matrix(na.omit(all_returns)) # drop the first row
```
```{r,include=FALSE}
pairs(all_returns)
```

```{r}
initial_wealth = 100000
sim1 = foreach(i=1:10000, .combine='rbind') %do% { #for each - rowbind each wealth tracker
	total_wealth = initial_wealth
	weights = c(0.4, 0.3, 0.3)
	holdings = weights * total_wealth
	n_days = 20
	wealthtracker = rep(0, n_days)
	for(today in 1:n_days) {
		return.today = resample(all_returns, 1, orig.ids=FALSE)
		holdings = holdings + holdings*return.today
		total_wealth = sum(holdings)
		wealthtracker[today] = total_wealth
		
		# rebalancing
	  weights = c(0.4, 0.3, 0.3)
	  holdings = weights * total_wealth
		
	}
	wealthtracker
}
```

```{r}
# Profit/loss
mean(sim1[,n_days]) 
hist(sim1[,n_days]- initial_wealth, breaks=30,
     main = "Histogram of Potential Profit/Loss - Portfolio 1",
     xlab = "Potential Profit/Loss",
     col = "light blue")
```

```{r}
# Calculate VaR
VaR5 = 100000 - quantile(sim1[,n_days], 0.05)
VaR5
```


## Portfolio 2:

- Large cap blend ETF: VTV (Vanguard Value ETF)
- Emerging market: GREK (Global X FTSE Greece 20 ETF)
- Asia Pacific: THD (iShares MSCI Thailand ETF)
- Fin. Services: KIE (SPDR S&P Insurance ETF)
```{r, warning=FALSE, message=FALSE, echo = FALSE}
rm(list=ls())
```

```{r}
# Import ETF
myETF2 = c("VTV","GREK","THD","KIE")
getSymbols(myETF2, from = "2014-08-01") # Last five years of data
```

```{r}
# Adjust for splits and dividends
for(ticker in myETF2) {
	expr = paste0(ticker, "a = adjustOHLC(", ticker, ")")
	eval(parse(text=expr))
}
# head(VTV)
```

```{r}
# Combine, in col, all the returns (close-to-close, 24-hr, changes) in a matrix
all_returns2 = cbind(ClCl(VTV),
								ClCl(GREK),
								ClCl(THD),
								ClCl(KIE))
# head(all_returns2)
all_returns2 = as.matrix(na.omit(all_returns2)) # drop the first row
```

```{r,include=FALSE}
pairs(all_returns2) # Large cap blend and financial services seem to be correlated
```

```{r}
initial_wealth = 100000
sim1 = foreach(i=1:10000, .combine='rbind') %do% { #for each - rowbind each wealth tracker
	total_wealth = initial_wealth
	weights = c(0.25, 0.25, 0.25, 0.25)
	holdings = weights * total_wealth
	n_days = 20
	wealthtracker = rep(0, n_days)
	for(today in 1:n_days) {
		return.today = resample(all_returns2, 1, orig.ids=FALSE)
		holdings = holdings + holdings*return.today
		total_wealth = sum(holdings)
		wealthtracker[today] = total_wealth
		
		# rebalancing
	  weights = c(0.25, 0.25, 0.25, 0.25)
	  holdings = weights * total_wealth
		
	}
	wealthtracker # record wealth of day 20
}
```

```{r}
# Profit/loss
mean(sim1[,n_days]) 
hist(sim1[,n_days]- initial_wealth, breaks=30,
     main = "Histogram of Potential Profit/Loss - Portfolio 2",
     xlab = "Potential Profit/Loss",
     col = "light green")
```

```{r}
# Calculate VaR
VaR5 = 100000 - quantile(sim1[,n_days], 0.05)
VaR5
```

## Porfolio 3:

- Emerging market: FRN (Invesco Frontier markets)
- Asia Pacific: ENZL (iShares MSCI New Zealand ETF)
- Technology: VGT (Vanguard Info Tech)
```{r, warning=FALSE, message=FALSE, echo = FALSE}
rm(list=ls())
```

```{r}
# Import ETF
myETF3 = c("FRN","ENZL","VGT")
getSymbols(myETF3, from = "2014-08-01") # Last five years of data
```

```{r}
# Adjust for splits and dividends
for(ticker in myETF3) {
	expr = paste0(ticker, "a = adjustOHLC(", ticker, ")")
	eval(parse(text=expr))
}
# head(FRN)
```

```{r}
# Combine, in col, all the returns (close-to-close, 24-hr, changes) in a matrix
all_returns3 = cbind(ClCl(FRN),
								ClCl(ENZL),
								ClCl(VGT))
# head(all_returns3)
all_returns3 = as.matrix(na.omit(all_returns3)) # drop the first row
```

```{r, include=FALSE}
pairs(all_returns3) #FRN and Tecnology seem to be correlated
```

```{r}
initial_wealth = 100000
sim1 = foreach(i=1:10000, .combine='rbind') %do% { #for each - rowbind each wealth tracker
	total_wealth = initial_wealth
	weights = c(0.2, 0.4, 0.4)
	holdings = weights * total_wealth
	n_days = 20
	wealthtracker = rep(0, n_days)
	for(today in 1:n_days) {
		return.today = resample(all_returns3, 1, orig.ids=FALSE)
		holdings = holdings + holdings*return.today
		total_wealth = sum(holdings)
		wealthtracker[today] = total_wealth
		
		# rebalancing
	  weights = c(0.2, 0.4, 0.4)
	  holdings = weights * total_wealth
		
	}
	wealthtracker
}
```

```{r}
# Profit/loss
mean(sim1[,n_days]) 
hist(sim1[,n_days]- initial_wealth, breaks=30,
     main = "Histogram of Potential Profit/Loss - Portfolio 3",
     xlab = "Potential Profit/Loss",
     col = "cyan")
```

```{r}
# Calculate VaR
VaR5 = 100000 - quantile(sim1[,n_days], 0.05)
VaR5
```

**Report:** Below is a brief description of each of our ETF portfolios, along with their values at right at the 5% level according to our simulations. The first two portfolios have relatively similar VaR at over $7k. Portfolio 3 is safer than the other two, at under  $5.5k of VaR. Note that given the above resampling was generated from Monte Carlo simulations, our answers will not be exactly replicable based on the code above.

**Portfolio 1:** Medium-High aggressiveness - This portfolio contains

1. Clymore CEF GS Connect ETN, a currently high-performing diversified ETF (i.e., it has the highest YTD growth among all diversified ETFs). Diversified ETFs are generally safer, though we increased the aggressiveness by choosing the one with the highest YTD growth.
2. iShares US Medical Devices ETF, a health & biotech ETF. The US health industry is one of the most lucrative in the world (albeit at the detriment of the American people) and continues to grow. While the volatility is high, the upside is as well.
3. First Trust Brazil Apha DEX Fund, a South American ETF. We decided to ensure more than one geography in each portfolio to guard against potential recession in any region, and picked a Brazilian one to go with the US for Portfolio 1. Though many still see Brazil as an emerging market, this particular ETF has been churning out high returns in recent years. Therefore, we believe it is a safe option to add to the portfolio.

This portfolio has a 5% VaR over $7.8k and the distribution of its returns is wide.

**Portfolio 2:** High aggressiveness - This portfolio contains

1. Vanguard Value ETF, a Large cap blend ETF. We have three other aggressive ETFs in this portfolio, so included a large cap/growth and value ETF to have a more reliable component for the portfolio. 
2. GREK (Global X FTSE Greece 20 ETF), an emerging market ETF. Greece, stabilizing post the bankruptcy and Grexit crisis, provides a good opportunity for high returns, though the risks are high as well.
3. iShares MSCI Thailand ETF, an Asia Pacific ETF. We added a different, yet still highly volatile geography into the mix.
4. SPDR S&P Insurance ETF, a financial services ETF. The financial services industry in general is highly lucrative, though the risks can be high as well (read: cause of the 2008 recession, high insurance losses from numerous natural disasters). We decided to include it, again, to be aggressive with the potential high returns.

This portfolio has a 5% VaR a bit under $7.6k, and the distribution of its returns is similar but slightly more concentrated than Portfolio 1.

**Portfolio 3:** Safe - This portfolio contains
1. iShares MSCI New Zealand ETF, an Asia Pacific ETF. This is a safe bet on a developed country with steady growth.
2. Invesco Frontier markets, an emerging market ETF. This is the only semi-aggressive ETF in the portfolio. That said, it is not focused on one single emerging market, so the risks are still diversified.
3. Vanguard Information Technology, a technology ETF. Technology is the present and the future. Including a technology ETF is vital in capturing and profiting from the growth of the world.

This portfolio has a 5% VaR of over $5.4k and the distribution of its returns is noticeably more concentrated than the two portfolios above.



# Problem 4. Market segmentation
```{r, warning=FALSE, message=FALSE}
rm(list=ls())
library(ggplot2)
library(LICORS)  # for kmeans++
library(foreach)
library(mosaic)

social = read.csv('social_marketing.csv', header=TRUE)
```

```{r}
x = social[,-1] #removes the first column
```
**Note:** Because the numbers of tweets by users vary greatly, we decided to scale the dataset by determining the proportions of each user's tweets that belong to different categories. That is, we want to use the % of tweets in each category rather than the absolute counts to cluster. 
```{r}
X = x/rowSums(x)
```

**k-means clustering**
```{r, include=FALSE}
# Using kmeans++ initialization to try 10 clusters
clust1 = kmeanspp(X, k=10, nstart=25)
clust1
```

**Choosing K**
```{r, warning=FALSE, message=FALSE}
k_grid = seq(2, 15, by = 1)
SSE_grid = foreach(k = k_grid, .combine = 'c') %do%
  {
    cluster_k = kmeanspp(X, k, nstart = 50)
    cluster_k$tot.withinss
  }
```

```{r}
plot(k_grid, SSE_grid)
# K = 7 looks like the elbow
```

```{r}
# Using kmeans++ initialization with 7 clusters
set.seed(10)
clust2 = kmeanspp(X, k=7, nstart=50)
clust2$centers
```

```{r}
clust2$size/7882
```

**Market Segment Identification:**
We clustered NutrientH2O's social media audience into seven clusters based on the percentage of tweets each user had in each category. Each cluster has at least 7% of the audience sample; so we feel each cluster is meaningful. We identified the following segments amongst the audience:

1. **The Health Gurus:** This segment (17% of the sample) is particularly interested in health_nutrition (22%) and personal fitness (11%). They also often discuss cooking (6%) and outdoors (4%).
2. **Regular College Kids:** This segment (16% of the sample) discusses a healthy amount of tv & film (7%), current events (6%), college events (5%), art (5%), music (3%), and dating (3%)
3. **The Worldly:** This segment (14% of the sample) includes those who regularly discuss politics (15)%, news (10%), and travel (9%). Interestingly, they also discuss more automative (5%) and computers (4%) than other segments. 
4. **Middle America:** This segment (14% of the sample) includes those who are particularly interested in watching sports (11% of tweets), religion (9%), and parenting (7%).
5. **College Gamers:** This segment (7% of the sample) often discusses online gaming (18%) and college-related events (19%).
6. **The Social Media Influencers:** This segment (10% of the sample) often tweets about cooking (19%), photo sharing (10%), fashion (9%), and beauty (6%).
7. **The Others:** This segment (24% of the sample) does not have many targeted interests. That said, they have many chatter tweets (24%), often shares photos (12%),and discuss a healthy amount of current events (5%) and shopping (7%).


# Problem 5. Author attribution

```{r warning=FALSE, message=FALSE}
# Setup
rm(list = ls())
library(tm) 
library(magrittr)
library(slam)
library(proxy)

readerPlain = function(fname){
  readPlain(elem=list(content=readLines(fname)), 
           id=fname, language='en') }
```

Prepare training set
```{r}
# Read in files
dir.list = list.files('ReutersC50/C50train/')
file_list2 = c()
authors = c()
for (x in dir.list){
  # read files into one DF, ensure to set up an author vector for response variable
  files = Sys.glob(paste('ReutersC50/C50train/', x,'/*.txt', sep=''))
  file_list2 = c(file_list2, files)
  authors = c(authors, rep(x,50))
}
```

```{r}
# clean up file names
all.authors = lapply(file_list2, readerPlain) 
mynames = file_list2 %>%
{ strsplit(., '/', fixed=TRUE) } %>%
{ lapply(., tail, n=2) } %>%
{ lapply(., paste0, collapse = '') } %>%
  unlist

# Rename all files
names(all.authors) = mynames
```

Set up document term matrix
```{r, warning = FALSE, message=FALSE}
# create a text mining corpus with the plain docs 
documents_raw = Corpus(VectorSource(all.authors))

# Pre-processing/tokenization step
my_documents = documents_raw
my_documents = tm_map(my_documents, content_transformer(tolower)) # make everything lowercase
my_documents = tm_map(my_documents, content_transformer(removeNumbers)) # remove numbers
my_documents = tm_map(my_documents, content_transformer(removePunctuation)) # remove punctuation
my_documents = tm_map(my_documents, content_transformer(stripWhitespace)) # remove excess white-space
my_documents = tm_map(my_documents, content_transformer(removeWords), stopwords("en")) # Remove stop words
```
```{R}
# create a doc-term-matrix
DTM_all.authors = DocumentTermMatrix(my_documents)
# DTM_all.authors # some basic summary statistics; ~#32.6k terms
DTM_all.authors = removeSparseTerms(DTM_all.authors, 0.95) # remove words not used in > 5% of docs
DTM_all.authors # now 801 terms
```

Construct TF-IDF weights
```{r}
tfidf_all.authors = weightTfIdf(DTM_all.authors)
```

Train a Random Forest model
```{r warning=FALSE, message=FALSE}
library(stringr)
library(klaR)
library(caret)
library(randomForest)

adtm.df<-as.data.frame(as.matrix(tfidf_all.authors))
adtm.df$authors=authors # add the response variable to the df
names(adtm.df) <- make.names(names(adtm.df))
set.seed(10)
model2 <- randomForest(as.factor(authors)~., data = adtm.df, ntree = 1000, mtry = 10, importance = TRUE)
```


Set up the test set - same steps as for the training set
```{r, warning = FALSE, message=FALSE}
dir.list = list.files('ReutersC50/C50test/')
file_list3 = c()
authors2 = c()
for (x in dir.list){
  files = Sys.glob(paste('ReutersC50/C50test/', x,'/*.txt', sep=''))
  file_list3 = c(file_list3, files)
  authors2 = c(authors2, rep(x,50))
}

all.authors2 = lapply(file_list3, readerPlain)
mynames = file_list3 %>%
  { strsplit(., '/', fixed=TRUE) } %>%
  { lapply(., tail, n=2) } %>%
  { lapply(., paste0, collapse = '') } %>%
  unlist

# Rename the articles
names(all.authors2) = mynames

# Create a text mining corpus with the plain docs
documents_raw = Corpus(VectorSource(all.authors2))

# Pre-processing/tokenization steps
my_documents = documents_raw
my_documents = tm_map(my_documents, content_transformer(tolower)) # make everything lowercase
my_documents = tm_map(my_documents, content_transformer(removeNumbers)) # remove numbers
my_documents = tm_map(my_documents, content_transformer(removePunctuation)) # remove punctuation
my_documents = tm_map(my_documents, content_transformer(stripWhitespace)) # remove excess white-space
my_documents = tm_map(my_documents, content_transformer(removeWords), stopwords("en")) # remove stop words
```

Create a doc-term-matrix for the test set
```{r}
DTM_all.authors2 = DocumentTermMatrix(my_documents)
# DTM_all.authors2 # ~33.4K terms
DTM_all.authors2 = removeSparseTerms(DTM_all.authors2, 0.95) # remove words not used in > 5% of docs
DTM_all.authors2 # now ~ 816 terms
```
**Note:** unlike Naive Bayes, Random Forest does not assume independence and compound probabilities. Therefore, terms not in the training set are simply not considered in the model and do not affect the existing terms' predictive power. As such, we did not set up a pseudo-word count for our model.

Construct TF-IDF weights
```{r}
tfidf_all.authors2 = weightTfIdf(DTM_all.authors2)
tfidf_all.authors2
```

Set up test set data frame and predict
```{r}
adtm.df_test2<-as.data.frame(as.matrix(tfidf_all.authors2))
rf.pred = predict(model2, data=adtm.df_test2)
mean(rf.pred==authors2) #80.32% accuracy
```

Deeper dive into model performance 
```{r}
rf.confusion.matrix = confusionMatrix(table(rf.pred,authors2))
rf.confusion.matrix$overall # again, 80.32% accuracy
rf.confusion.matrix.df = as.data.frame(rf.confusion.matrix$byClass)
rf.confusion.matrix.df[order(-rf.confusion.matrix.df$Sensitivity),1:2]
```

**Report:** To predict authorship based on an article’s textual content,  we relied on term TF-IDFs as the predictors and  tested several Random Forest models. We reached the best accuracy of 80% with 1000 trees and 10 variables.
Our data pre-processing and analysis pipeline is as followed (the same steps were followed for both training and test data):

1. Read the file names and author names in as two vectors
2. Clean up each file’s name by splitting the file on “/,” keeping only the last two parts of the name (Author Name and unique identifier), and concatenate those two parts
3. Read the files with the readPlain function and name each file the cleaned version of its name  (from Step 2)
4. Turn the plain documents into a vector then a corpus for text mining
5. Tokenize the symbols in the corpus by
    i. Making everything lowercase
    ii. Removing numbers
    iii. Removing punctuations
    iv. Removing excess white spaces
    v. Removing stop words
6. Create a document term matrix (DTM) and remove words that are not used in over 5% of the documents in the corpus
7. Calculate the TF-IDF for each term in each document of the DTM with the weightTfIdf function – the TF-IDF weights would serve as the features for our Random Forest models. We use the training set’s TF-IDF weights to train the model. Then use the model on the test set’s TF-IDF weights to predict authorship.

As mentioned above, our best Random Forest model had 1000 trees and sampled 10 variables at each split. The model reached an overall accuracy of 80%. Additionally, when predicting for a specific author, our model’s precision (specificity) is generally over 99%. That said, some  authors’ recalls (sensitivity, or true positives out of all positives) were especially bad. For example, the model only identified about a third of all Scott Hillis’ articles.


# Problem 6. Association rule mining  

We first read in the data and create the appropriate model
```{r, warning= FALSE, message=FALSE}
rm(list=ls())
library(tidyverse)
library(arules)
library(arulesViz)
groceries <- read.transactions('groceries.txt', sep=',')
groceries_trans = as(groceries, "transactions")
grocery_rules = apriori(groceries, 
                     parameter=list(support=.005, confidence=.1, maxlen=3))
```
We can then run various imspections to get a clearer idea of the associations. We also look at different subsets, controlling for lift, confidence, and both lift and confidence.

```{r include = FALSE}
# Exploratory
inspect(grocery_rules)

inspect(subset(grocery_rules, subset=lift > 3))
inspect(subset(grocery_rules, subset=confidence > 0.4))
inspect(subset(grocery_rules, subset=lift > 2 & confidence > 0.5))
```

We then plot the confidence, support, and lift to visualize the spreads of the metrics and see if there are relationships among them.
```{r message=FALSE}
plot(grocery_rules)
```

Based on the graph showing the spreads of support, confidence, and lift above, we decided to

- not filter by support, as most data points have lower than 0.05 support
- filter for > 0.5 in confidence, which should still leave a decent sample size of data points, then
- further filter for > 3 in lift to ensure meaningful rules

We then use the filters and plot the association map below.
```{r}
sub1 = subset(grocery_rules, subset=confidence > 0.50 & lift > 2.5)
inspect(sub1)
```

```{r}
plot(sub1, method='graph')
```

**Rule Interpretation:**
Among the 28 rules we generated (i.e., those we're confident in and have high lifts), most have "other vegetables" on the right-hand side, while three have "whole milk" and one has "yogurt." As can be seen from the graph above

- The "other vegetables" rules make general sense as the other vegetables are usually connected with specific vegetables and fruits (additionally, they are generally in the same section at a grocery story), which are always in the left-hand side of the rules predicting "other vegetables." 
- The "whole milk" rules also make sense. "Whole milk" is connected to many dairy products in the graph above, understandably as most people who cook with dairy products require many types of such products. And as expected,  all rules have some sort of dairy product on the left-hand side.
- Lastly, the "yogurt" rule (predicted from curd and tropical fruit) makes sense as curd is also a dairy product, connected to yogurt in the graph above, and people often put fruits with their yogurt.  
